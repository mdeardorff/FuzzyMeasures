{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choquet_integral import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "import math\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.stats import entropy\n",
    "import random\n",
    "import xai_indices as xai\n",
    "import pyemd\n",
    "from sklearn import datasets\n",
    "from emd_clustertend import vat, ivat, emd_pairwise_dissimilarity\n",
    "from sklearn.preprocessing import scale\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'crisp.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0e370608a99a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mpart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m \u001b[0mcrisp_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_crisp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"crisp.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;31m#input: crisp partition matrix, reordering from odm to dm, odm itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0e370608a99a>\u001b[0m in \u001b[0;36mread_crisp\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_crisp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'crisp.csv'"
     ]
    }
   ],
   "source": [
    "def random_node_fm(n):\n",
    "    ch = ChoquetIntegral()\n",
    "    ch.type='quad'\n",
    "    ch.fm = {}\n",
    "    ch.fm[str(np.arange(1,n+1))] = 1\n",
    "    ch.fm['[]'] = 0\n",
    "    ch.N = n\n",
    "    ch.M = n\n",
    "    numkeys = len(ch.get_keys_index())\n",
    "    keys = list(ch.get_keys_index().keys())\n",
    "    keys.append('[]')\n",
    "    s = [0] * (numkeys + 1)\n",
    "    s[-1] = 1\n",
    "    s[-2] = 1\n",
    "    done = False\n",
    "    while not done:\n",
    "        randindex = random.randrange(0,numkeys)\n",
    "        if s[randindex] == 0:\n",
    "            s[randindex] = 1\n",
    "            if keys[randindex] != '[]':\n",
    "                compare_key = [int(s) for s in keys[randindex][1:-1].split() if s.isdigit()]\n",
    "\n",
    "            else:\n",
    "                compare_key = []\n",
    "\n",
    "            maxi = 0\n",
    "            max_index = 0\n",
    "            mini = 1\n",
    "            min_index = 0\n",
    "            for i,key in enumerate(keys):\n",
    "                if s[i] == 1 and i != randindex:\n",
    "                    if key != '[]':\n",
    "                        stripped = [int(s) for s in key[1:-1].split() if s.isdigit()]\n",
    "                    else:\n",
    "                        stripped = []\n",
    "\n",
    "                    if is_subset(stripped,compare_key) and s[i] == 1 and ch.fm[key] >= maxi:\n",
    "                        maxi = ch.fm[key]\n",
    "                        max_index = i\n",
    "                    if is_subset(compare_key,stripped) and s[i] == 1 and ch.fm[key] <= mini:\n",
    "\n",
    "                        mini = ch.fm[key]\n",
    "                        min_index = i\n",
    "\n",
    "\n",
    "            rb = ch.fm[keys[max_index]]\n",
    "            ru = ch.fm[keys[min_index]]\n",
    "            g = random.uniform(rb,ru)\n",
    "            ch.fm[keys[randindex]] = g\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        if min(s) == 1:\n",
    "            done = True\n",
    "    return ch\n",
    "\n",
    "# a is subset of b\n",
    "def is_subset(a,b):\n",
    "    if len(a) == 0:\n",
    "        return True\n",
    "    if len(b) == 0 and len(a) != 0:\n",
    "        return False\n",
    "    for val in a:\n",
    "        if val not in b:\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def sample_with_noise(ch,data,mean,var):\n",
    "    labels = np.zeros(data.shape[0])\n",
    "    for i,point in enumerate(data):\n",
    "        labels[i] = max(min(ch.chi_quad(point) + random.gauss(mean,var),1),0)\n",
    "    return labels\n",
    "\n",
    "def gen_datapoints(m,n):\n",
    "    points = []\n",
    "    for i in range(n):\n",
    "        point = []\n",
    "        for j in range(m):\n",
    "            point.append(random.random())\n",
    "        points.append(point)\n",
    "    return np.asarray(points)\n",
    "\n",
    "def percentage_walks_observed(walks):\n",
    "    seen = 0\n",
    "    total = 0\n",
    "    for key in walks.keys():\n",
    "        if walks[key] > 0:\n",
    "            seen = seen + 1\n",
    "        total = total+1\n",
    "    return seen/total\n",
    "\n",
    "def read_crisp(fname):\n",
    "    part = []\n",
    "    with open(fname) as csvfile:\n",
    "        reader = csv.reader(csvfile,delimiter=',')\n",
    "        for row in reader:\n",
    "            part.append(row)\n",
    "    return np.asarray(part,dtype=int)\n",
    "crisp_part = read_crisp(\"crisp.csv\")\n",
    "\n",
    "#input: crisp partition matrix, reordering from odm to dm, odm itself\n",
    "def center_from_crisp(crisp,reordering,odm):\n",
    "    n = crisp.shape[0]\n",
    "    row = 0\n",
    "    num_in_cluster = 0\n",
    "    cluster_centers = []\n",
    "    for i in range(n):\n",
    "        if crisp[row][i] == 1:\n",
    "            num_in_cluster = num_in_cluster + 1\n",
    "            \n",
    "        else:\n",
    "#             print(\"new cluster at\", i)\n",
    "            mini = np.inf\n",
    "            mini_row = row\n",
    "            for j in range(row,row+num_in_cluster):\n",
    "                row_sum = np.sum(odm[j][i-num_in_cluster:i])\n",
    "                if row_sum < mini:\n",
    "                    mini = row_sum\n",
    "                    mini_row = j\n",
    "            cluster_centers.append(reorder[mini_row][0][0])   #the center is row w/ minimum difference, put back in unordered\n",
    "            row = row+num_in_cluster\n",
    "            num_in_cluster = 1\n",
    "            \n",
    "    mini = np.inf\n",
    "    mini_row = row\n",
    "    for j in range(row,row+num_in_cluster):\n",
    "        row_sum = np.sum(odm[j][i-num_in_cluster+1:i+1])\n",
    "#         print(row_sum, \"sum of row\", j)\n",
    "        if row_sum < mini:\n",
    "            mini = row_sum\n",
    "            mini_row = j\n",
    "    cluster_centers.append(reorder[mini_row][0][0])\n",
    "    return cluster_centers\n",
    "\n",
    "def generate_walk_diffs(chi):\n",
    "    n = chi.M\n",
    "    initial = []\n",
    "    for i in range(n):\n",
    "        initial.append(i+1)\n",
    "\n",
    "    index_permutations = itertools.permutations(initial,n)\n",
    "    walks = np.ndarray((math.factorial(n),n),dtype=int)\n",
    "    walk_diffs = np.ndarray((math.factorial(n),n),dtype=float)\n",
    "    for i,val in enumerate(index_permutations):\n",
    "        walks[i] = np.asarray(val)\n",
    "\n",
    "        c_build = np.ndarray(n)\n",
    "        for j in range(len(walks[i])):\n",
    "            if j == 0:\n",
    "                c_build[j] = chi.fm[str(walks[i][0:1])]\n",
    "            else:\n",
    "                c_build[j] = (chi.fm[str(np.sort(walks[i][0:j+1]))]) - (chi.fm[str(np.sort(walks[i][0:j]))])\n",
    "\n",
    "        walk_diffs[i] = c_build\n",
    "    return walk_diffs\n",
    "\n",
    "\n",
    "def gen_seen_walk_diffs(walk_visitation,ch):\n",
    "    seen_walks = []\n",
    "    for walk in walk_visitation.keys():\n",
    "        if walk_visitation[walk] > 0:\n",
    "            seen_walks.append(walk)\n",
    "    \n",
    "    n = len(seen_walks[0])\n",
    "    walk_diffs = np.ndarray((len(seen_walks),n),dtype=float)\n",
    "    for i,walk in enumerate(seen_walks):\n",
    "        walk = list(walk)\n",
    "        c_build = np.ndarray(n)\n",
    "        for j in range(len(walk)):\n",
    "            if j == 0:\n",
    "                c_build[j] = ch.fm[str(walk[0:1])]\n",
    "            else:\n",
    "                c_build[j] = ch.fm[str(np.sort(walk[0:j+1]))] - ch.fm[str(np.sort(walk[0:j]))]\n",
    "        walk_diffs[i] = c_build\n",
    "    return walk_diffs\n",
    "\n",
    "def gen_seen_walk_diffs_2(walk_visitation,ch):\n",
    "    seen_walks = []\n",
    "    for walk in walk_visitation:\n",
    "        if walk_visitation[walk] > 0:\n",
    "            seen_walks.append(walk)\n",
    "    \n",
    "    n = len(seen_walks[0])\n",
    "    walk_diffs = np.ndarray((len(seen_walks),n),dtype=float)\n",
    "    for i,walk in enumerate(seen_walks):\n",
    "        walk = list(walk)\n",
    "        c_build = np.ndarray(n)\n",
    "        for j in range(len(walk)):\n",
    "            if j == 0:\n",
    "                c_build[j] = ch.fm[str(walk[0:1])]\n",
    "            else:\n",
    "                c_build[j] = ch.fm[str(np.sort(walk[0:j+1]))] - ch.fm[str(np.sort(walk[0:j]))]\n",
    "        walk_diffs[i] = c_build\n",
    "    return walk_diffs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_owa = ChoquetIntegral()\n",
    "split_owa.N = 5\n",
    "split_fm = {}\n",
    "split_fm[str(np.asarray([1]))] = .1\n",
    "split_fm[str(np.asarray([2]))] = .1\n",
    "split_fm[str(np.asarray([3]))] = .1\n",
    "split_fm[str(np.asarray([4]))] = .1\n",
    "split_fm[str(np.asarray([5]))] = .1\n",
    "split_fm[str(np.asarray([1,2]))] = .2\n",
    "split_fm[str(np.asarray([1,3]))] = .2\n",
    "split_fm[str(np.asarray([1,4]))] = .2\n",
    "split_fm[str(np.asarray([1,5]))] = .2\n",
    "split_fm[str(np.asarray([2,3]))] = .2\n",
    "split_fm[str(np.asarray([2,4]))] = .4\n",
    "split_fm[str(np.asarray([2,5]))] = .4\n",
    "split_fm[str(np.asarray([3,4]))] = .4\n",
    "split_fm[str(np.asarray([3,5]))] = .4\n",
    "split_fm[str(np.asarray([4,5]))] = .4\n",
    "split_fm[str(np.asarray([1,2,3]))] = .6\n",
    "split_fm[str(np.asarray([1,2,4]))] = .6\n",
    "split_fm[str(np.asarray([1,2,5]))] = .6\n",
    "split_fm[str(np.asarray([1,3,4]))] = .6\n",
    "split_fm[str(np.asarray([1,3,5]))] = .6\n",
    "split_fm[str(np.asarray([1,4,5]))] = .6\n",
    "split_fm[str(np.asarray([2,3,4]))] = .6\n",
    "split_fm[str(np.asarray([2,3,5]))] = .6\n",
    "split_fm[str(np.asarray([2,4,5]))] = .6\n",
    "split_fm[str(np.asarray([3,4,5]))] = .6\n",
    "split_fm[str(np.asarray([1,2,3,4]))] = .6\n",
    "split_fm[str(np.asarray([1,2,3,5]))] = .6\n",
    "split_fm[str(np.asarray([1,2,4,5]))] = .6\n",
    "split_fm[str(np.asarray([1,3,4,5]))] = .9\n",
    "split_fm[str(np.asarray([2,3,4,5]))] = .9\n",
    "split_fm[str(np.asarray([1,2,3,4,5]))] = 1\n",
    "split_owa.fm = split_fm\n",
    "split_owa.type ='quad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_owa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a39ad8ed36de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_datapoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnoisy_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_with_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_owa\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m.025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnoisy_split_owa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChoquetIntegral\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_owa' is not defined"
     ]
    }
   ],
   "source": [
    "data = gen_datapoints(5,300)\n",
    "noisy_labels = sample_with_noise(split_owa,data,0,.025)\n",
    "\n",
    "noisy_split_owa = ChoquetIntegral()\n",
    "print(data.shape)\n",
    "noisy_split_owa.train_chi(data.T,noisy_labels)\n",
    "\n",
    "(walks,_) = xai.walk_visitation(data.T)\n",
    "# print(walks)\n",
    "print(percentage_walks_observed(walks))\n",
    "\n",
    "owa_diffs = split_owa.generate_walk_diffs()\n",
    "ivat(owa_diffs,figure_size=(5,5))\n",
    "noisy_owa_diffs = noisy_split_owa.generate_walk_diffs()\n",
    "odm = ivat(noisy_owa_diffs,figure_size=(5,5),return_odm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10694444444444444\n"
     ]
    }
   ],
   "source": [
    "#Read in the variables for a FM\n",
    "\n",
    "airfoil_bias = 0\n",
    "with open(\"yacht.csv\",'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    fm_vars = []\n",
    "    for row in reader:\n",
    "        fm_vars.append(row[1])\n",
    "    airfoil_bias = fm_vars[-1]\n",
    "\n",
    "airfoil_noreg_bias = 0\n",
    "with open(\"yacht_noreg.csv\",'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    noreg_fm_vars = []\n",
    "    for row in reader:\n",
    "        noreg_fm_vars.append(row[1])\n",
    "    airfoil_noreg_bias = noreg_fm_vars[-1]\n",
    "        \n",
    "with open(\"yacht_sort.csv\",'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    sorts = []\n",
    "    for row in reader:\n",
    "        introw = [int(x) for x in row]\n",
    "        sorts.append(introw)\n",
    "\n",
    "\n",
    "sorts = [tuple(sort) for sort in sorts]\n",
    "print(len(set(sorts)) / math.factorial(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 7)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create choquet integral object, create FM\n",
    "airfoil_ch = ChoquetIntegral()\n",
    "airfoil_ch.N = 6\n",
    "airfoil_fm = {}\n",
    "for i,key in enumerate(airfoil_ch.get_keys_index()):\n",
    "    airfoil_fm[key] = float(fm_vars[i+1])\n",
    "    \n",
    "airfoil_ch.fm = airfoil_fm\n",
    "\n",
    "#Enumerate walks and generate array of LOSs\n",
    "airfoil_diffs = airfoil_ch.generate_walk_diffs()\n",
    "\n",
    "airfoil_bias_diffs = np.zeros((np.shape(airfoil_diffs)[0],np.shape(airfoil_diffs)[1]+1))\n",
    "\n",
    "for i,diff in enumerate(airfoil_diffs):\n",
    "    airfoil_bias_diffs[i] = np.append(diff,airfoil_bias)\n",
    "    \n",
    "airfoil_bias_diffs.shape\n",
    "\n",
    "# print(airfoil_diffs)\n",
    "\n",
    "#dissimilarity matrix can be created by metrics.pairwise_distances(airfoil_diffs)\n",
    "# i.e. airfoil_diffs is shape (120,5), dissimilarity matrix would be (120,120)\n",
    "\n",
    "# ivat(airfoil_diffs,euclidean=True)\n",
    "#the python ivat library we found sucks (not symmetric), so we have to export to matlab to generate plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "airfoil_noreg = ChoquetIntegral()\n",
    "airfoil_noreg.N = 6\n",
    "airfoil_noreg.get_keys_index()\n",
    "airfoil_noreg_fm = {}\n",
    "for i,key in enumerate(airfoil_noreg.get_keys_index()):\n",
    "    airfoil_noreg_fm[key] = float(noreg_fm_vars[i+1])\n",
    "    \n",
    "airfoil_noreg.fm = airfoil_noreg_fm\n",
    "\n",
    "airfoil_noreg_diffs = airfoil_noreg.generate_walk_diffs()\n",
    "\n",
    "\n",
    "airfoil_bias_noreg_diffs = np.zeros((np.shape(airfoil_diffs)[0],np.shape(airfoil_diffs)[1]+1))\n",
    "\n",
    "for i,diff in enumerate(airfoil_noreg_diffs):\n",
    "    airfoil_bias_noreg_diffs[i] = np.append(diff,airfoil_noreg_bias)\n",
    "    \n",
    "\n",
    "# ivat(airfoil_noreg_diffs,euclidean=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_seen_walk_diffs_2(seen_walks,ch):\n",
    "\n",
    "    n = len(seen_walks[0])\n",
    "    walk_diffs = np.ndarray((len(seen_walks),n),dtype=float)\n",
    "    for i,walk in enumerate(seen_walks):\n",
    "        walk = list(walk)\n",
    "        c_build = np.ndarray(n)\n",
    "        for j in range(len(walk)):\n",
    "            if j == 0:\n",
    "                c_build[j] = ch.fm[str(walk[0:1])]\n",
    "            else:\n",
    "                c_build[j] = ch.fm[str(np.sort(walk[0:j+1]))] - ch.fm[str(np.sort(walk[0:j]))]\n",
    "        walk_diffs[i] = c_build\n",
    "    return walk_diffs\n",
    "\n",
    "\n",
    "airfoil_seen_diss = metrics.pairwise_distances(gen_seen_walk_diffs_2(list(set(sorts)),airfoil_ch))\n",
    "airfoil_noreg_seen_diss = metrics.pairwise_distances(gen_seen_walk_diffs_2(list(set(sorts)),airfoil_noreg))\n",
    "\n",
    "airfoil_all_diss = metrics.pairwise_distances(airfoil_bias_diffs)\n",
    "airfoil_noreg_all_diss = metrics.pairwise_distances(airfoil_bias_noreg_diffs)\n",
    "\n",
    "\n",
    "airfoil_mat = {}\n",
    "airfoil_mat[\"yacht_seen_diss\"] = airfoil_seen_diss\n",
    "airfoil_mat[\"yacht_noreg_seen_diss\"] = airfoil_noreg_seen_diss\n",
    "airfoil_mat[\"yacht_all_diss\"] = airfoil_all_diss\n",
    "airfoil_mat[\"yacht_noreg_all_diss\"] = airfoil_noreg_all_diss\n",
    "sio.savemat(\"yacht.mat\",airfoil_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
