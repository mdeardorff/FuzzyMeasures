{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choquet_integral import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "import math\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.stats import entropy\n",
    "import random\n",
    "import xai_indices as xai\n",
    "import pyemd\n",
    "from sklearn import datasets\n",
    "from emd_clustertend import vat, ivat\n",
    "from sklearn.preprocessing import scale\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_node_fm(n):\n",
    "    ch = ChoquetIntegral()\n",
    "    ch.type='quad'\n",
    "    ch.fm = {}\n",
    "    ch.fm[str(np.arange(1,n+1))] = 1\n",
    "    ch.fm['[]'] = 0\n",
    "    ch.N = n\n",
    "    ch.M = n\n",
    "    numkeys = len(ch.get_keys_index())\n",
    "    keys = list(ch.get_keys_index().keys())\n",
    "    keys.append('[]')\n",
    "    s = [0] * (numkeys + 1)\n",
    "    s[-1] = 1\n",
    "    s[-2] = 1\n",
    "    done = False\n",
    "    while not done:\n",
    "        randindex = random.randrange(0,numkeys)\n",
    "        if s[randindex] == 0:\n",
    "            s[randindex] = 1\n",
    "            if keys[randindex] != '[]':\n",
    "                compare_key = [int(s) for s in keys[randindex][1:-1].split() if s.isdigit()]\n",
    "\n",
    "            else:\n",
    "                compare_key = []\n",
    "\n",
    "            maxi = 0\n",
    "            max_index = 0\n",
    "            mini = 1\n",
    "            min_index = 0\n",
    "            for i,key in enumerate(keys):\n",
    "                if s[i] == 1 and i != randindex:\n",
    "                    if key != '[]':\n",
    "                        stripped = [int(s) for s in key[1:-1].split() if s.isdigit()]\n",
    "                    else:\n",
    "                        stripped = []\n",
    "\n",
    "                    if is_subset(stripped,compare_key) and s[i] == 1 and ch.fm[key] >= maxi:\n",
    "                        maxi = ch.fm[key]\n",
    "                        max_index = i\n",
    "                    if is_subset(compare_key,stripped) and s[i] == 1 and ch.fm[key] <= mini:\n",
    "\n",
    "                        mini = ch.fm[key]\n",
    "                        min_index = i\n",
    "\n",
    "\n",
    "            rb = ch.fm[keys[max_index]]\n",
    "            ru = ch.fm[keys[min_index]]\n",
    "            g = random.uniform(rb,ru)\n",
    "            ch.fm[keys[randindex]] = g\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        if min(s) == 1:\n",
    "            done = True\n",
    "    return ch\n",
    "\n",
    "# a is subset of b\n",
    "def is_subset(a,b):\n",
    "    if len(a) == 0:\n",
    "        return True\n",
    "    if len(b) == 0 and len(a) != 0:\n",
    "        return False\n",
    "    for val in a:\n",
    "        if val not in b:\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def sample_with_noise(ch,data,mean,var):\n",
    "    labels = np.zeros(data.shape[0])\n",
    "    for i,point in enumerate(data):\n",
    "        labels[i] = max(min(ch.chi_quad(point) + random.gauss(mean,var),1),0)\n",
    "    return labels\n",
    "\n",
    "def gen_datapoints(m,n):\n",
    "    points = []\n",
    "    for i in range(n):\n",
    "        point = []\n",
    "        for j in range(m):\n",
    "            point.append(random.random())\n",
    "        points.append(point)\n",
    "    return np.asarray(points)\n",
    "\n",
    "def percentage_walks_observed(walks):\n",
    "    seen = 0\n",
    "    total = 0\n",
    "    for key in walks.keys():\n",
    "        if walks[key] > 0:\n",
    "            seen = seen + 1\n",
    "        total = total+1\n",
    "    return seen/total\n",
    "\n",
    "def read_crisp(fname):\n",
    "    part = []\n",
    "    with open(fname) as csvfile:\n",
    "        reader = csv.reader(csvfile,delimiter=',')\n",
    "        for row in reader:\n",
    "            part.append(row)\n",
    "    return np.asarray(part,dtype=int)\n",
    "crisp_part = read_crisp(\"crisp.csv\")\n",
    "\n",
    "#input: crisp partition matrix, reordering from odm to dm, odm itself\n",
    "def center_from_crisp(crisp,reordering,odm):\n",
    "    n = crisp.shape[0]\n",
    "    row = 0\n",
    "    num_in_cluster = 0\n",
    "    cluster_centers = []\n",
    "    for i in range(n):\n",
    "        if crisp[row][i] == 1:\n",
    "            num_in_cluster = num_in_cluster + 1\n",
    "            \n",
    "        else:\n",
    "#             print(\"new cluster at\", i)\n",
    "            mini = np.inf\n",
    "            mini_row = row\n",
    "            for j in range(row,row+num_in_cluster):\n",
    "                row_sum = np.sum(odm[j][i-num_in_cluster:i])\n",
    "                if row_sum < mini:\n",
    "                    mini = row_sum\n",
    "                    mini_row = j\n",
    "            cluster_centers.append(reorder[mini_row][0][0])   #the center is row w/ minimum difference, put back in unordered\n",
    "            row = row+num_in_cluster\n",
    "            num_in_cluster = 1\n",
    "            \n",
    "    mini = np.inf\n",
    "    mini_row = row\n",
    "    for j in range(row,row+num_in_cluster):\n",
    "        row_sum = np.sum(odm[j][i-num_in_cluster+1:i+1])\n",
    "#         print(row_sum, \"sum of row\", j)\n",
    "        if row_sum < mini:\n",
    "            mini = row_sum\n",
    "            mini_row = j\n",
    "    cluster_centers.append(reorder[mini_row][0][0])\n",
    "    return cluster_centers\n",
    "\n",
    "def generate_walk_diffs(chi):\n",
    "    n = chi.M\n",
    "    initial = []\n",
    "    for i in range(n):\n",
    "        initial.append(i+1)\n",
    "\n",
    "    index_permutations = itertools.permutations(initial,n)\n",
    "    walks = np.ndarray((math.factorial(n),n),dtype=int)\n",
    "    walk_diffs = np.ndarray((math.factorial(n),n),dtype=float)\n",
    "    for i,val in enumerate(index_permutations):\n",
    "        walks[i] = np.asarray(val)\n",
    "\n",
    "        c_build = np.ndarray(n)\n",
    "        for j in range(len(walks[i])):\n",
    "            if j == 0:\n",
    "                c_build[j] = chi.fm[str(walks[i][0:1])]\n",
    "            else:\n",
    "                c_build[j] = (chi.fm[str(np.sort(walks[i][0:j+1]))]) - (chi.fm[str(np.sort(walks[i][0:j]))])\n",
    "\n",
    "        walk_diffs[i] = c_build\n",
    "    return walk_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_seen_walk_diffs(walk_visitation,ch):\n",
    "    seen_walks = []\n",
    "    for walk in walk_visitation.keys():\n",
    "        if walk_visitation[walk] > 0:\n",
    "            seen_walks.append(walk)\n",
    "    \n",
    "    n = len(seen_walks[0])\n",
    "    walk_diffs = np.ndarray((len(seen_walks),n),dtype=float)\n",
    "    for i,walk in enumerate(seen_walks):\n",
    "        walk = list(walk)\n",
    "        c_build = np.ndarray(n)\n",
    "        for j in range(len(walk)):\n",
    "            if j == 0:\n",
    "                c_build[j] = ch.fm[str(walk[0:1])]\n",
    "            else:\n",
    "                c_build[j] = ch.fm[str(np.sort(walk[0:j+1]))] - ch.fm[str(np.sort(walk[0:j]))]\n",
    "        walk_diffs[i] = c_build\n",
    "    return walk_diffs\n",
    "        \n",
    "    \n",
    "(walks,_) = xai.walk_visitation(data.T)\n",
    "seen_diffs = gen_seen_walk_diffs(walks,noisy_split_owa)\n",
    "all_diffs = noisy_split_owa.generate_walk_diffs()\n",
    "\n",
    "ivat(all_diffs,figure_size=(5,5),euclidean=True)\n",
    "ivat(all_diffs,figure_size=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = ChoquetIntegral()\n",
    "custom.N = 4\n",
    "customfm = {}\n",
    "customfm[str(np.asarray([1]))] = .9\n",
    "customfm[str(np.asarray([2]))] = 0\n",
    "customfm[str(np.asarray([3]))] = 0\n",
    "customfm[str(np.asarray([4]))] = 0\n",
    "customfm[str(np.asarray([1,2]))] = .9\n",
    "customfm[str(np.asarray([1,3]))] = .9\n",
    "customfm[str(np.asarray([1,4]))] = .9\n",
    "customfm[str(np.asarray([2,3]))] = 0\n",
    "customfm[str(np.asarray([2,4]))] = 0\n",
    "customfm[str(np.asarray([3,4]))] = 0\n",
    "customfm[str(np.asarray([1,2,3]))] = .9\n",
    "customfm[str(np.asarray([1,2,4]))] = .9\n",
    "customfm[str(np.asarray([1,3,4]))] = .9\n",
    "customfm[str(np.asarray([2,3,4]))] = 0\n",
    "customfm[str(np.asarray([1,2,3,4]))] = 1\n",
    "\n",
    "custom.fm = customfm\n",
    "custom.type='quad'\n",
    "print(custom.fm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_owa = ChoquetIntegral()\n",
    "split_owa.N = 5\n",
    "split_fm = {}\n",
    "split_fm[str(np.asarray([1]))] = .1\n",
    "split_fm[str(np.asarray([2]))] = .1\n",
    "split_fm[str(np.asarray([3]))] = .1\n",
    "split_fm[str(np.asarray([4]))] = .1\n",
    "split_fm[str(np.asarray([5]))] = .1\n",
    "split_fm[str(np.asarray([1,2]))] = .2\n",
    "split_fm[str(np.asarray([1,3]))] = .2\n",
    "split_fm[str(np.asarray([1,4]))] = .2\n",
    "split_fm[str(np.asarray([1,5]))] = .2\n",
    "split_fm[str(np.asarray([2,3]))] = .2\n",
    "split_fm[str(np.asarray([2,4]))] = .4\n",
    "split_fm[str(np.asarray([2,5]))] = .4\n",
    "split_fm[str(np.asarray([3,4]))] = .4\n",
    "split_fm[str(np.asarray([3,5]))] = .4\n",
    "split_fm[str(np.asarray([4,5]))] = .4\n",
    "split_fm[str(np.asarray([1,2,3]))] = .6\n",
    "split_fm[str(np.asarray([1,2,4]))] = .6\n",
    "split_fm[str(np.asarray([1,2,5]))] = .6\n",
    "split_fm[str(np.asarray([1,3,4]))] = .6\n",
    "split_fm[str(np.asarray([1,3,5]))] = .6\n",
    "split_fm[str(np.asarray([1,4,5]))] = .6\n",
    "split_fm[str(np.asarray([2,3,4]))] = .6\n",
    "split_fm[str(np.asarray([2,3,5]))] = .6\n",
    "split_fm[str(np.asarray([2,4,5]))] = .6\n",
    "split_fm[str(np.asarray([3,4,5]))] = .6\n",
    "split_fm[str(np.asarray([1,2,3,4]))] = .6\n",
    "split_fm[str(np.asarray([1,2,3,5]))] = .6\n",
    "split_fm[str(np.asarray([1,2,4,5]))] = .6\n",
    "split_fm[str(np.asarray([1,3,4,5]))] = .9\n",
    "split_fm[str(np.asarray([2,3,4,5]))] = .9\n",
    "split_fm[str(np.asarray([1,2,3,4,5]))] = 1\n",
    "split_owa.fm = split_fm\n",
    "split_owa.type ='quad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min = ChoquetIntegral()\n",
    "max_min.N = 5\n",
    "max_min.M = 5\n",
    "max_min_fm = {}\n",
    "max_min_fm['[]'] = 0\n",
    "max_min_fm[str(np.asarray([1]))] = .1\n",
    "max_min_fm[str(np.asarray([2]))] = 0\n",
    "max_min_fm[str(np.asarray([3]))] = 0\n",
    "max_min_fm[str(np.asarray([4]))] = 0\n",
    "max_min_fm[str(np.asarray([5]))] = 0\n",
    "max_min_fm[str(np.asarray([1,2]))] = .1\n",
    "max_min_fm[str(np.asarray([1,3]))] = .1\n",
    "max_min_fm[str(np.asarray([1,4]))] = .1\n",
    "max_min_fm[str(np.asarray([1,5]))] = .1\n",
    "max_min_fm[str(np.asarray([2,3]))] = 0\n",
    "max_min_fm[str(np.asarray([2,4]))] = 0\n",
    "max_min_fm[str(np.asarray([2,5]))] = 0\n",
    "max_min_fm[str(np.asarray([3,4]))] = 0\n",
    "max_min_fm[str(np.asarray([3,5]))] = 0\n",
    "max_min_fm[str(np.asarray([4,5]))] = 0\n",
    "max_min_fm[str(np.asarray([1,2,3]))] = .1\n",
    "max_min_fm[str(np.asarray([1,2,4]))] = .1\n",
    "max_min_fm[str(np.asarray([1,2,5]))] = .1\n",
    "max_min_fm[str(np.asarray([1,3,4]))] = .1\n",
    "max_min_fm[str(np.asarray([1,3,5]))] = .1\n",
    "max_min_fm[str(np.asarray([1,4,5]))] = .1\n",
    "max_min_fm[str(np.asarray([2,3,4]))] = 0\n",
    "max_min_fm[str(np.asarray([2,3,5]))] = 0\n",
    "max_min_fm[str(np.asarray([2,4,5]))] = 0\n",
    "max_min_fm[str(np.asarray([3,4,5]))] = 0\n",
    "max_min_fm[str(np.asarray([1,2,3,4]))] = .1001\n",
    "max_min_fm[str(np.asarray([1,2,3,5]))] = .1001\n",
    "max_min_fm[str(np.asarray([1,2,4,5]))] = .1001\n",
    "max_min_fm[str(np.asarray([1,3,4,5]))] = .1001\n",
    "max_min_fm[str(np.asarray([2,3,4,5]))] = 0\n",
    "max_min_fm[str(np.asarray([1,2,3,4,5]))] = 1\n",
    "max_min.fm = max_min_fm\n",
    "max_min.type ='quad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min.fm\n",
    "max_min_diffs = max_min.generate_walk_diffs()\n",
    "ivat(max_min_diffs)\n",
    "ivat(max_min_diffs,euclidean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gen_datapoints(5,300)\n",
    "noisy_labels = sample_with_noise(split_owa,data,0,0)\n",
    "\n",
    "noisy_split_owa = ChoquetIntegral()\n",
    "noisy_split_owa.train_chi(data.T,noisy_labels)\n",
    "\n",
    "(walks,_) = xai.walk_visitation(data.T)\n",
    "# print(walks)\n",
    "print(percentage_walks_observed(walks))\n",
    "\n",
    "owa_diffs = split_owa.generate_walk_diffs()\n",
    "ivat(owa_diffs,figure_size=(5,5))\n",
    "noisy_owa_diffs = noisy_split_owa.generate_walk_diffs()\n",
    "odm = ivat(noisy_owa_diffs,figure_size=(5,5),return_odm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(odm,cmap=\"gray\")\n",
    "plt.savefig(\"split_owa_no_error.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gen_datapoints(4,200)\n",
    "noisy_labels = sample_with_noise(split_owa,data,0,0)\n",
    "\n",
    "noisy_chi = ChoquetIntegral()\n",
    "noisy_chi.train_chi(data.T,noisy_labels)\n",
    "\n",
    "xai.walk_visitation(data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(walks,_) = xai.walk_visitation(data.T)\n",
    "\n",
    "\n",
    "percentage_walks_observed(walks)\n",
    "# print(walks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ch = random_node_fm(6)\n",
    "\n",
    "diffs = ch.generate_walk_diffs()\n",
    "odm = ivat(diffs,return_odm=True,figure_size=(5,5),euclidean=False)\n",
    "\n",
    "# odm,reorder = ivat(diffs,return_odm=True,euclidean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(odm,cmap='gray')\n",
    "plt.savefig(\"6var_struct\")\n",
    "# print(center_from_crisp(crisp_part,reorder,odm))\n",
    "# ch.generate_walk_diffs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"split_owa_noisy_.025.png\")\n",
    "y,x = img.shape[0], img.shape[1]\n",
    "img = img[10:y-10,75:x-75]\n",
    "cv2.imwrite(\"split_owa_noisy_.025.png\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('odm.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    for row in odm:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for g in range(1,2**7):\n",
    "    key = []\n",
    "    for i,val in enumerate(format(g,'b')[::-1]):\n",
    "        if val == '1':\n",
    "            key.append(i+1)\n",
    "    keys.append(np.asarray(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_fm = {}\n",
    "\n",
    "with open('FMs/fmvars_shared_0_0.txt', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i,row in enumerate(reader):\n",
    "        learner_fm[str(keys[i])] = float(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with diffs\n"
     ]
    }
   ],
   "source": [
    "learner_ch = ChoquetIntegral()\n",
    "learner_ch.N = 7\n",
    "learner_ch.M = 7\n",
    "learner_ch.type = 'quad'\n",
    "learner_ch.fm = learner_fm\n",
    "learner_diffs = learner_ch.generate_walk_diffs()\n",
    "print(\"done with diffs\")\n",
    "# ivat(learner_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"big_diffs.csv\",'w') as csvfile:\n",
    "    writer = csv.writer(csvfile,delimiter=',')\n",
    "    for i in range(diff_mat.shape[0]):\n",
    "        writer.writerow(diff_mat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff_mat = metrics.pairwise_distances(learner_diffs)\n",
    "mat_dict = {}\n",
    "mat_dict[\"diffs\"] = diff_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat(\"diffs.mat\",mat_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
